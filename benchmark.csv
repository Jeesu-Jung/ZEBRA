Model,Instruction-Following Evaluation(IFeval),MMLU,MMLU-pro,Hellaswag,ARC-easy,ARC-Challenge
gpt-4,85.37,86.40%,0.6371,95.30%,96.63%,96.40%
gpt-3.5-turbo,72.54,70.00%,0.3764,85.00%,92.80%,83.02%
bard,89.33,71.80%,0.5912,84.70%,84.43%,77.13%
llama-2-7b-chat,25.19,53.10%,0.2032,67.50%,72.14%,54.61%
llama-2-13b-chat,24.82,57.80%,0.2534,71.20%,72.05%,58.02%
llama-2-70b-chat,24.07,68.90%,0.3753,78.10%,82.20%,67.66%
ultralm-13b,54.92,49.58%,0.1938,54.00%,57.58%,48.04%
ultralm-65b,,,,,,
wizardlm-7b,45.83,42.50%,0.1845,77.70%,39.48%,34.90%
wizardlm-13b,33.92,52.30%,0.1688,81.00%,72.94%,55.38%
wizardlm-70b,49.51,52.70%,0.2718,83.30%,80.68%,71.93%
vicuna-33b,52.76,64%,0.2309,75.00%,81.57%,64.51%
alpaca-7b,30.58,37.92%,0.1464,23.60%,43.31%,33.79%
falcon-40b-instruct,24.54,67.50%,0.1402,80.00%,76.60%,56.70%
mpt-30b-chat,30.7,50.40%,0.2037,24.53%,87.12%,70.73%
starchat,28.3,40.12%,0.118,25.40%,16.96%,9.07%
pythia-12b,24.71,27%,0.121,25.60%,24.49%,31.80%
